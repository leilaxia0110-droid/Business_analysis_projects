# Customer Churn Prediction
This project builds machine learning models to predict customer churn.

## Models
Logistic Regression (baseline)
K-Nearest Neighbors
Random Forest

## Evaluation
Models are evaluated using Accuracy, Precision, Recall, and ROC-AUC.
Random Forest achieved the best overall performance after hyperparameter tuning.

## Output
ROC curve comparison
Saved best model

# Customer Churn Prediction

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    confusion_matrix, roc_curve, auc)

import joblib

# 1. Load data (example placeholder)

# Replace this with real data loading
# df = pd.read_csv("churn_data.csv")
# X = df.drop("Churn", axis=1)
# y = df["Churn"]

# Dummy example (for structure demonstration)
X = pd.DataFrame({
    "age": [25, 45, 35, 50, 23, 40],
    "balance": [5000, 20000, 12000, 30000, 2000, 18000],
    "active": [1, 0, 1, 0, 1, 0]})
y = pd.Series([0, 1, 0, 1, 0, 1])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)


# 2. Train baseline models

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier(random_state=42)}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    results[name] = {
        "model": model,
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, zero_division=0),
        "recall": recall_score(y_test, y_pred, zero_division=0),
        "auc": roc_auc,
        "fpr": fpr,
        "tpr": tpr}


# 3. Hyperparameter tuning (Random Forest)

param_grid = {
    "n_estimators": [80, 100],
    "max_depth": [5, 10, None],
    "min_samples_split": [2, 5]}

grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring="f1",
    n_jobs=-1)
grid_rf.fit(X_train, y_train)

best_rf = grid_rf.best_estimator_

y_pred_rf = best_rf.predict(X_test)
y_proba_rf = best_rf.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)
auc_rf = auc(fpr_rf, tpr_rf)


# 4. Plot ROC curves

plt.figure(figsize=(8, 6))
plt.plot([0, 1], [0, 1], "k--", label="Random Guess")

for name, r in results.items():
    plt.plot(r["fpr"], r["tpr"], label=f"{name} (AUC={r['auc']:.2f})")

plt.plot(fpr_rf, tpr_rf, linewidth=2,
         label=f"Tuned RF (AUC={auc_rf:.2f})")

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.tight_layout()
plt.savefig("roc_curve.png", dpi=300)
plt.show()


# 5. Save best model

joblib.dump(best_rf, "best_churn_model.pkl")

print("Churn prediction modeling completed.")
print("Best model: Tuned Random Forest")
